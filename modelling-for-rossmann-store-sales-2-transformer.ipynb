{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14209460,"sourceType":"datasetVersion","datasetId":9063408,"isSourceIdPinned":false}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transformer Architecture for Time Series:\n\n- Self-attention mechanism captures long-range dependencies\n- Positional encoding preserves sequence order\n- More parallelizable than LSTM (faster training)\n- Better at capturing complex patterns in longer sequences","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# Device configuration for PyTorch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Reference date for time-based calculations\nREFERENCE_DATE = pd.Timestamp(\"2015-01-01\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:31:33.243142Z","iopub.execute_input":"2026-01-05T12:31:33.243317Z","iopub.status.idle":"2026-01-05T12:31:37.898133Z","shell.execute_reply.started":"2026-01-05T12:31:33.243298Z","shell.execute_reply":"2026-01-05T12:31:37.897340Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\nTRAIN_DF_PATH = kagglehub.dataset_download(\"sachinchaudhary123/rossman-fe\")\nTRAIN_FILE_NAME = \"/train_with_clusters.csv\"\n \n# Load dataset from feature engineering\ntrain = pd.read_csv(TRAIN_DF_PATH+TRAIN_FILE_NAME, index_col=\"Date\", parse_dates=True)\ndisplay(train.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:31:37.899632Z","iopub.execute_input":"2026-01-05T12:31:37.900014Z","iopub.status.idle":"2026-01-05T12:31:41.956527Z","shell.execute_reply.started":"2026-01-05T12:31:37.899962Z","shell.execute_reply":"2026-01-05T12:31:41.955874Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_55/2585745017.py:8: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n  train = pd.read_csv(TRAIN_DF_PATH+TRAIN_FILE_NAME, index_col=\"Date\", parse_dates=True)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"            Store  DayOfWeek  Sales  Customers  Open  Promo StateHoliday  \\\nDate                                                                       \n2015-07-31      1          4   5263        555     1      1            0   \n2015-07-31      2          4   6064        625     1      1            0   \n2015-07-31      3          4   8314        821     1      1            0   \n2015-07-31      4          4  13995       1498     1      1            0   \n2015-07-31      5          4   4822        559     1      1            0   \n\n            SchoolHoliday  SalePerCustomer  LogSales  ...  IsSchoolHoliday  \\\nDate                                                  ...                    \n2015-07-31              1         9.482883  8.568646  ...                1   \n2015-07-31              1         9.702400  8.710290  ...                1   \n2015-07-31              1        10.126675  9.025816  ...                1   \n2015-07-31              1         9.342457  9.546527  ...                1   \n2015-07-31              1         8.626118  8.481151  ...                1   \n\n            Sales_Rolling_Mean_7  Sales_Rolling_Mean_14  \\\nDate                                                      \n2015-07-31                   NaN                    NaN   \n2015-07-31                   NaN                    NaN   \n2015-07-31                   NaN                    NaN   \n2015-07-31                   NaN                    NaN   \n2015-07-31                   NaN                    NaN   \n\n            Sales_Rolling_Mean_28  Sales_Lag_7  Sales_Lag_14  Sales_Lag_21  \\\nDate                                                                         \n2015-07-31                    NaN          NaN           NaN           NaN   \n2015-07-31                    NaN          NaN           NaN           NaN   \n2015-07-31                    NaN          NaN           NaN           NaN   \n2015-07-31                    NaN          NaN           NaN           NaN   \n2015-07-31                    NaN          NaN           NaN           NaN   \n\n            Sales_Lag_28  Lag_SalePerCustomer_7day  StoreCluster  \nDate                                                              \n2015-07-31           NaN                       NaN             0  \n2015-07-31           NaN                       NaN             1  \n2015-07-31           NaN                       NaN             1  \n2015-07-31           NaN                       NaN             0  \n2015-07-31           NaN                       NaN             2  \n\n[5 rows x 25 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Store</th>\n      <th>DayOfWeek</th>\n      <th>Sales</th>\n      <th>Customers</th>\n      <th>Open</th>\n      <th>Promo</th>\n      <th>StateHoliday</th>\n      <th>SchoolHoliday</th>\n      <th>SalePerCustomer</th>\n      <th>LogSales</th>\n      <th>...</th>\n      <th>IsSchoolHoliday</th>\n      <th>Sales_Rolling_Mean_7</th>\n      <th>Sales_Rolling_Mean_14</th>\n      <th>Sales_Rolling_Mean_28</th>\n      <th>Sales_Lag_7</th>\n      <th>Sales_Lag_14</th>\n      <th>Sales_Lag_21</th>\n      <th>Sales_Lag_28</th>\n      <th>Lag_SalePerCustomer_7day</th>\n      <th>StoreCluster</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2015-07-31</th>\n      <td>1</td>\n      <td>4</td>\n      <td>5263</td>\n      <td>555</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>9.482883</td>\n      <td>8.568646</td>\n      <td>...</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2015-07-31</th>\n      <td>2</td>\n      <td>4</td>\n      <td>6064</td>\n      <td>625</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>9.702400</td>\n      <td>8.710290</td>\n      <td>...</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2015-07-31</th>\n      <td>3</td>\n      <td>4</td>\n      <td>8314</td>\n      <td>821</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>10.126675</td>\n      <td>9.025816</td>\n      <td>...</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2015-07-31</th>\n      <td>4</td>\n      <td>4</td>\n      <td>13995</td>\n      <td>1498</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>9.342457</td>\n      <td>9.546527</td>\n      <td>...</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2015-07-31</th>\n      <td>5</td>\n      <td>4</td>\n      <td>4822</td>\n      <td>559</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>8.626118</td>\n      <td>8.481151</td>\n      <td>...</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 25 columns</p>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"def rmspe(y_true, y_pred):\n    \"\"\"\n    Calculate Root Mean Square Percentage Error (Kaggle metric).\n\n    Args:\n        y_true: Actual values (numpy array)\n        y_pred: Predicted values (numpy array)\n\n    Returns:\n        RMSPE score (float)\n\n    Note: Only considers samples where y_true > 0\n    \"\"\"\n    # Convert to numpy if tensor\n    if isinstance(y_true, torch.Tensor):\n        y_true = y_true.cpu().numpy()\n    if isinstance(y_pred, torch.Tensor):\n        y_pred = y_pred.cpu().numpy()\n\n    # Flatten arrays\n    y_true = np.array(y_true).flatten()\n    y_pred = np.array(y_pred).flatten()\n\n    # Filter out zero sales (as per Kaggle rules)\n    mask = y_true > 0\n    y_true_filtered = y_true[mask]\n    y_pred_filtered = y_pred[mask]\n\n    if len(y_true_filtered) == 0:\n        return 0.0\n\n    # Calculate percentage errors\n    pct_errors = (y_true_filtered - y_pred_filtered) / y_true_filtered\n\n    # Calculate RMSPE\n    rmspe_score = np.sqrt(np.mean(pct_errors ** 2))\n\n    return rmspe_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:31:41.957320Z","iopub.execute_input":"2026-01-05T12:31:41.957594Z","iopub.status.idle":"2026-01-05T12:31:41.963173Z","shell.execute_reply.started":"2026-01-05T12:31:41.957561Z","shell.execute_reply":"2026-01-05T12:31:41.962382Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    \"\"\"\n    Injects positional information into the sequence.\n    \n    PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n    \"\"\"\n    \n    def __init__(self, d_model, max_len=5000, dropout=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        # Create positional encoding matrix\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        \n        div_term = torch.exp(\n            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n        )\n        \n        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n        \n        pe = pe.unsqueeze(0)  # Add batch dimension: (1, max_len, d_model)\n        \n        # Register as buffer (not a parameter, but part of state)\n        self.register_buffer('pe', pe)\n    \n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Tensor of shape (batch_size, seq_len, d_model)\n        Returns:\n            Tensor with positional encoding added\n        \"\"\"\n        x = x + self.pe[:, :x.size(1), :]\n        return self.dropout(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:31:41.963970Z","iopub.execute_input":"2026-01-05T12:31:41.964233Z","iopub.status.idle":"2026-01-05T12:31:41.982284Z","shell.execute_reply.started":"2026-01-05T12:31:41.964214Z","shell.execute_reply":"2026-01-05T12:31:41.981541Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import time\n\nstart_time = time.time()\n\n# Features for Transformer\ntransformer_features = [\n    \"Sales\", \"Promo\", \"IsStateHoliday\",\n    \"IsSchoolHoliday\", \"Sales_Lag_7\", \"Sales_Rolling_Mean_7\"\n]\n\n# Create fresh copy with only needed columns\ntransformer_df = train[transformer_features + [\"Store\", \"StoreCluster\"]].copy()\n\n# Step 1: Drop NaN rows (required for sequence creation anyway)\ntransformer_clean = transformer_df.dropna(subset=transformer_features)\nprint(f\"Original rows: {len(transformer_df):,}\")\nprint(f\"After dropna: {len(transformer_clean):,}\")\n\n# Step 2: Fit Sales Scaler (for inverse transform)\nsales_scaler = MinMaxScaler()\nsales_scaler.fit(transformer_clean[[\"Sales\"]])\nprint(f\"\\nSales Range: {transformer_clean['Sales'].min():,.0f} to {transformer_clean['Sales'].max():,.0f}\")\n\n# Step 3: Scale ALL features at once using a single scaler\nfeature_scaler = MinMaxScaler()\ntransformer_clean[transformer_features] = feature_scaler.fit_transform(\n    transformer_clean[transformer_features]\n)\n\nelapsed = time.time() - start_time\nprint(f\"\\nScaling complete in {elapsed:.2f} seconds!\")\nprint(f\"Scaled data shape: {transformer_clean.shape}\")\n\ndisplay(transformer_clean.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:31:41.984020Z","iopub.execute_input":"2026-01-05T12:31:41.984308Z","iopub.status.idle":"2026-01-05T12:31:42.181350Z","shell.execute_reply.started":"2026-01-05T12:31:41.984275Z","shell.execute_reply":"2026-01-05T12:31:42.180748Z"}},"outputs":[{"name":"stdout","text":"Original rows: 1,017,209\nAfter dropna: 1,009,404\n\nSales Range: 0 to 41,551\n\nScaling complete in 0.17 seconds!\nScaled data shape: (1009404, 8)\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/3037528430.py:26: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  transformer_clean[transformer_features] = feature_scaler.fit_transform(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"               Sales  Promo  IsStateHoliday  IsSchoolHoliday  Sales_Lag_7  \\\nDate                                                                        \n2015-07-24  0.089192    0.0             0.0              0.0     0.126664   \n2015-07-24  0.092753    0.0             0.0              1.0     0.145941   \n2015-07-24  0.122259    0.0             0.0              1.0     0.200091   \n2015-07-24  0.200284    0.0             0.0              1.0     0.336815   \n2015-07-24  0.091815    0.0             0.0              1.0     0.116050   \n\n            Sales_Rolling_Mean_7  Store  StoreCluster  \nDate                                                   \n2015-07-24              0.029059      1             0  \n2015-07-24              0.045787      2             1  \n2015-07-24              0.071610      3             1  \n2015-07-24              0.133684      4             0  \n2015-07-24              0.147202      5             2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sales</th>\n      <th>Promo</th>\n      <th>IsStateHoliday</th>\n      <th>IsSchoolHoliday</th>\n      <th>Sales_Lag_7</th>\n      <th>Sales_Rolling_Mean_7</th>\n      <th>Store</th>\n      <th>StoreCluster</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2015-07-24</th>\n      <td>0.089192</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.126664</td>\n      <td>0.029059</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2015-07-24</th>\n      <td>0.092753</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.145941</td>\n      <td>0.045787</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2015-07-24</th>\n      <td>0.122259</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.200091</td>\n      <td>0.071610</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2015-07-24</th>\n      <td>0.200284</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.336815</td>\n      <td>0.133684</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2015-07-24</th>\n      <td>0.091815</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.116050</td>\n      <td>0.147202</td>\n      <td>5</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"\ntransformer_features = [\n    \"Sales\", \"Promo\", \"IsStateHoliday\",\n    \"IsSchoolHoliday\", \"Sales_Lag_7\", \"Sales_Rolling_Mean_7\"\n]\n\n# Get required columns\ncols_needed = transformer_features + [\"Store\", \"StoreCluster\"]\ntransformer_df = train[cols_needed].copy()\n\n# Drop NaN\nprint(f\"\\nOriginal rows: {len(transformer_df):,}\")\ntransformer_clean = transformer_df.dropna()\nprint(f\"After dropna: {len(transformer_clean):,}\")\n\n\n# PROPER SCALING - Scale Features before Creating Sequences\n\n# Create a copy for scaling\nscaled_df = transformer_clean.copy()\n\n# Scale ALL features together\nfeature_scaler = MinMaxScaler()\nscaled_df[transformer_features] = feature_scaler.fit_transform(\n    scaled_df[transformer_features]\n)\n\n# Verify scaling worked\nprint(f\"\\After Scaling:\")\nprint(f\"  Features min: {scaled_df[transformer_features].min().min():.4f}\")\nprint(f\"  Features max: {scaled_df[transformer_features].max().max():.4f}\")\n\n# Should be 0.0 to 1.0\nassert scaled_df[transformer_features].min().min() >= 0.0, \"Min should be >= 0\"\nassert scaled_df[transformer_features].max().max() <= 1.0, \"Max should be <= 1\"\nprint(\"All features properly scaled to [0, 1]\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:31:42.182192Z","iopub.execute_input":"2026-01-05T12:31:42.182495Z","iopub.status.idle":"2026-01-05T12:31:42.491053Z","shell.execute_reply.started":"2026-01-05T12:31:42.182463Z","shell.execute_reply":"2026-01-05T12:31:42.490228Z"}},"outputs":[{"name":"stdout","text":"\nOriginal rows: 1,017,209\nAfter dropna: 1,009,404\n","output_type":"stream"},{"name":"stderr","text":"<>:28: SyntaxWarning: invalid escape sequence '\\A'\n<>:28: SyntaxWarning: invalid escape sequence '\\A'\n/tmp/ipykernel_55/3842275440.py:28: SyntaxWarning: invalid escape sequence '\\A'\n  print(f\"\\After Scaling:\")\n","output_type":"stream"},{"name":"stdout","text":"\\After Scaling:\n  Features min: 0.0000\n  Features max: 1.0000\nAll features properly scaled to [0, 1]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def create_sequences_with_dates(df, features, target, window=30):\n    \"\"\"Create sequences with date tracking for analysis.\"\"\"\n    X, y, store_ids, dates = [], [], [], []\n    \n    for store_id, store_df in df.groupby(\"Store\"):\n        store_df = store_df.dropna().sort_index()\n        values = store_df[features].values\n        targets = store_df[target].values\n        date_index = store_df.index\n        \n        for i in range(window, len(store_df)):\n            X.append(values[i-window:i])\n            y.append(targets[i])\n            store_ids.append(store_id)\n            dates.append(date_index[i])\n    \n    return np.array(X), np.array(y), np.array(store_ids), dates\n\nWINDOW_SIZE = 30\n\nX_trans, y_trans, store_ids_trans, date_indices_trans = create_sequences_with_dates(\n    scaled_df, transformer_features, \"Sales\", WINDOW_SIZE\n)\n\n# Get cluster IDs\ncluster_lookup = (\n    train[[\"Store\", \"StoreCluster\"]]\n    .drop_duplicates()\n    .set_index(\"Store\")[\"StoreCluster\"]\n)\ncluster_ids_trans = np.array([cluster_lookup[s] for s in store_ids_trans])\n\nprint(f\"Transformer Dataset:\")\nprint(f\"   Sequences: {X_trans.shape[0]:,}\")\nprint(f\"   Window size: {X_trans.shape[1]}\")\nprint(f\"   Features: {X_trans.shape[2]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:31:42.492085Z","iopub.execute_input":"2026-01-05T12:31:42.492360Z","iopub.status.idle":"2026-01-05T12:31:51.969353Z","shell.execute_reply.started":"2026-01-05T12:31:42.492337Z","shell.execute_reply":"2026-01-05T12:31:51.968611Z"}},"outputs":[{"name":"stdout","text":"Transformer Dataset:\n   Sequences: 975,954\n   Window size: 30\n   Features: 6\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"split_idx = int(len(X_trans) * 0.8)\n\nX_tr = X_trans[:split_idx]\nX_va = X_trans[split_idx:]\ny_tr = y_trans[:split_idx]\ny_va = y_trans[split_idx:]\nc_tr = cluster_ids_trans[:split_idx]\nc_va = cluster_ids_trans[split_idx:]\ndates_va = date_indices_trans[split_idx:]\nstores_va = store_ids_trans[split_idx:]\n\nprint(f\"Split Summary:\")\nprint(f\"   Training: {len(X_tr):,} samples\")\nprint(f\"   Validation: {len(X_va):,} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:31:51.970242Z","iopub.execute_input":"2026-01-05T12:31:51.970452Z","iopub.status.idle":"2026-01-05T12:31:51.978931Z","shell.execute_reply.started":"2026-01-05T12:31:51.970432Z","shell.execute_reply":"2026-01-05T12:31:51.977925Z"}},"outputs":[{"name":"stdout","text":"Split Summary:\n   Training: 780,763 samples\n   Validation: 195,191 samples\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"class TransformerDataset(Dataset):\n    \"\"\"Dataset for Transformer model.\"\"\"\n    \n    def __init__(self, X, y, cluster_ids):\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.float32)\n        self.cluster_ids = torch.tensor(cluster_ids, dtype=torch.long)\n    \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        return self.X[idx], self.cluster_ids[idx], self.y[idx]\n\nBATCH_SIZE = 128\n\ntrain_ds = TransformerDataset(X_tr, y_tr, c_tr)\nval_ds = TransformerDataset(X_va, y_va, c_va)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=3)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=3)\n\nprint(f\"DataLoader Info:\")\nprint(f\"   Training batches: {len(train_loader)}\")\nprint(f\"   Validation batches: {len(val_loader)}\")\n\nnum_features = X_tr.shape[2]\nnum_clusters = int(cluster_ids_trans.max() + 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:31:51.979904Z","iopub.execute_input":"2026-01-05T12:31:51.980197Z","iopub.status.idle":"2026-01-05T12:31:52.344870Z","shell.execute_reply.started":"2026-01-05T12:31:51.980174Z","shell.execute_reply":"2026-01-05T12:31:52.344144Z"}},"outputs":[{"name":"stdout","text":"DataLoader Info:\n   Training batches: 6100\n   Validation batches: 1525\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"class SalesTransformer(nn.Module):\n    \"\"\"\n    Transformer model for sales forecasting with cluster embeddings.\n    \n    Architecture:\n    1. Input projection: (num_features) -> (d_model)\n    2. Positional encoding: Add position information\n    3. Transformer encoder: Self-attention layers\n    4. Cluster embedding: Store cluster information\n    5. Output projection: Predict sales\n    \"\"\"\n    \n    def __init__(\n        self,\n        num_features,\n        d_model=64,\n        nhead=4,\n        num_encoder_layers=2,\n        dim_feedforward=128,\n        dropout=0.1,\n        num_clusters=4,\n        cluster_embed_dim=8\n    ):\n        super().__init__()\n        \n        self.d_model = d_model\n        \n        # Input projection layer\n        self.input_projection = nn.Linear(num_features, d_model)\n        \n        # Positional encoding\n        self.pos_encoder = PositionalEncoding(d_model, dropout=dropout)\n        \n        # Transformer encoder layers\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True  # Input shape: (batch, seq, features)\n        )\n        self.transformer_encoder = nn.TransformerEncoder(\n            encoder_layer,\n            num_layers=num_encoder_layers\n        )\n        \n        # Cluster embedding\n        self.cluster_embedding = nn.Embedding(num_clusters, cluster_embed_dim)\n        \n        # Output layers\n        self.fc = nn.Sequential(\n            nn.Linear(d_model + cluster_embed_dim, 64),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1)\n        )\n        \n        # Initialize weights\n        self._init_weights()\n    \n    def _init_weights(self):\n        \"\"\"Initialize weights for better convergence.\"\"\"\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n    \n    def forward(self, x, cluster_id, src_mask=None):\n        \"\"\"\n        Args:\n            x: Input tensor (batch_size, seq_len, num_features)\n            cluster_id: Store cluster IDs (batch_size,)\n            src_mask: Optional attention mask\n        \n        Returns:\n            Predicted sales (batch_size,)\n        \"\"\"\n        # Project input to d_model dimensions\n        x = self.input_projection(x) * math.sqrt(self.d_model)\n        \n        # Add positional encoding\n        x = self.pos_encoder(x)\n        \n        # Pass through transformer encoder\n        x = self.transformer_encoder(x, src_mask)\n        \n        # Use the last time step's output (like LSTM)\n        x = x[:, -1, :]  # (batch_size, d_model)\n        \n        # Get cluster embedding\n        cluster_emb = self.cluster_embedding(cluster_id)  # (batch_size, cluster_embed_dim)\n        \n        # Concatenate and predict\n        combined = torch.cat([x, cluster_emb], dim=1)\n        output = self.fc(combined)\n        \n        return output.squeeze(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:31:52.346030Z","iopub.execute_input":"2026-01-05T12:31:52.346295Z","iopub.status.idle":"2026-01-05T12:31:52.359913Z","shell.execute_reply.started":"2026-01-05T12:31:52.346273Z","shell.execute_reply":"2026-01-05T12:31:52.359270Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"transformer_model = SalesTransformer(\n        num_features=num_features,\n        d_model=64,\n        nhead=4,\n        num_encoder_layers=2,\n        dim_feedforward=128,\n        dropout=0.1,\n        num_clusters=num_clusters,\n        cluster_embed_dim=8\n    ).to(device)\n\nprint(\"Using: SalesTransformer\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:31:52.361420Z","iopub.execute_input":"2026-01-05T12:31:52.361617Z","iopub.status.idle":"2026-01-05T12:31:52.564316Z","shell.execute_reply.started":"2026-01-05T12:31:52.361598Z","shell.execute_reply":"2026-01-05T12:31:52.563683Z"}},"outputs":[{"name":"stdout","text":"Using: SalesTransformer\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Print model summary\nprint(\"TRANSFORMER MODEL ARCHITECTURE\")\nprint(transformer_model)\n\ntotal_params = sum(p.numel() for p in transformer_model.parameters())\ntrainable_params = sum(p.numel() for p in transformer_model.parameters() if p.requires_grad)\nprint(f\"Parameters: {total_params:,} total, {trainable_params:,} trainable\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:31:52.565117Z","iopub.execute_input":"2026-01-05T12:31:52.565414Z","iopub.status.idle":"2026-01-05T12:31:52.570656Z","shell.execute_reply.started":"2026-01-05T12:31:52.565389Z","shell.execute_reply":"2026-01-05T12:31:52.569932Z"}},"outputs":[{"name":"stdout","text":"TRANSFORMER MODEL ARCHITECTURE\nSalesTransformer(\n  (input_projection): Linear(in_features=6, out_features=64, bias=True)\n  (pos_encoder): PositionalEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (transformer_encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0-1): 2 x TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n        )\n        (linear1): Linear(in_features=64, out_features=128, bias=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n        (linear2): Linear(in_features=128, out_features=64, bias=True)\n        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.1, inplace=False)\n        (dropout2): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (cluster_embedding): Embedding(4, 8)\n  (fc): Sequential(\n    (0): Linear(in_features=72, out_features=64, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.1, inplace=False)\n    (3): Linear(in_features=64, out_features=32, bias=True)\n    (4): ReLU()\n    (5): Linear(in_features=32, out_features=1, bias=True)\n  )\n)\nParameters: 74,209 total, 74,209 trainable\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"criterion = nn.MSELoss()\noptimizer = torch.optim.AdamW(\n    transformer_model.parameters(),\n    lr=0.001,\n    weight_decay=0.01  # L2 regularization\n)\n\n# Learning rate scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode='min',\n    factor=0.5,\n    patience=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:31:52.571547Z","iopub.execute_input":"2026-01-05T12:31:52.571778Z","iopub.status.idle":"2026-01-05T12:31:54.996283Z","shell.execute_reply.started":"2026-01-05T12:31:52.571757Z","shell.execute_reply":"2026-01-05T12:31:54.995673Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"EPOCHS = 10\nbest_val_loss = float('inf')\npatience_counter = 0\nEARLY_STOPPING_PATIENCE = 7\n\ntrain_losses = []\nval_losses = []\nval_rmses = []\n\nprint(\"TRAINING TRANSFORMER\")\n\nfor epoch in range(EPOCHS):\n    #  Training Phase \n    transformer_model.train()\n    epoch_train_losses = []\n    \n    for X_batch, c_batch, y_batch in train_loader:\n        X_batch = X_batch.to(device)\n        c_batch = c_batch.to(device)\n        y_batch = y_batch.to(device)\n        \n        # Forward pass\n        optimizer.zero_grad()\n        predictions = transformer_model(X_batch, c_batch)\n        loss = criterion(predictions, y_batch)\n        \n        # Backward pass\n        loss.backward()\n        \n        # Gradient clipping (important for transformers)\n        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        epoch_train_losses.append(loss.item())\n    \n    avg_train_loss = np.mean(epoch_train_losses)\n    train_losses.append(avg_train_loss)\n    \n    # Validation Phase \n    transformer_model.eval()\n    epoch_val_losses = []\n    val_preds_scaled = []\n    val_true_scaled = []\n    \n    with torch.no_grad():\n        for X_batch, c_batch, y_batch in val_loader:\n            X_batch = X_batch.to(device)\n            c_batch = c_batch.to(device)\n            y_batch = y_batch.to(device)\n            \n            predictions = transformer_model(X_batch, c_batch)\n            loss = criterion(predictions, y_batch)\n            \n            epoch_val_losses.append(loss.item())\n            val_preds_scaled.extend(predictions.cpu().numpy())\n            val_true_scaled.extend(y_batch.cpu().numpy())\n    \n    avg_val_loss = np.mean(epoch_val_losses)\n    val_losses.append(avg_val_loss)\n    \n    # Calculate RMSE (scaled)\n    scaled_rmse = np.sqrt(mean_squared_error(val_true_scaled, val_preds_scaled))\n    val_rmses.append(scaled_rmse)\n    \n    # Learning rate scheduling\n    scheduler.step(avg_val_loss)\n    \n    # Early stopping check\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        patience_counter = 0\n        # Save best model\n        best_model_state = transformer_model.state_dict().copy()\n    else:\n        patience_counter += 1\n    \n    # Print progress\n    current_lr = optimizer.param_groups[0]['lr']\n    print(f\"Epoch {epoch+1:2d}/{EPOCHS} | \"\n          f\"Train Loss: {avg_train_loss:.4f} | \"\n          f\"Val Loss: {avg_val_loss:.4f} | \"\n          f\"Val RMSE: {scaled_rmse:.4f} | \"\n          f\"LR: {current_lr:.6f}\")\n    \n    # Early stopping\n    if patience_counter >= EARLY_STOPPING_PATIENCE:\n        print(f\"Early stopping triggered at epoch {epoch+1}\")\n        break\n\n# Load best model\ntransformer_model.load_state_dict(best_model_state)\nprint(f\"Loaded best model (Val Loss: {best_val_loss:.4f})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:31:54.998894Z","iopub.execute_input":"2026-01-05T12:31:54.999357Z","iopub.status.idle":"2026-01-05T12:40:29.025440Z","shell.execute_reply.started":"2026-01-05T12:31:54.999332Z","shell.execute_reply":"2026-01-05T12:40:29.024581Z"}},"outputs":[{"name":"stdout","text":"TRAINING TRANSFORMER\nEpoch  1/10 | Train Loss: 0.0010 | Val Loss: 0.0001 | Val RMSE: 0.0079 | LR: 0.001000\nEpoch  2/10 | Train Loss: 0.0001 | Val Loss: 0.0000 | Val RMSE: 0.0068 | LR: 0.001000\nEpoch  3/10 | Train Loss: 0.0001 | Val Loss: 0.0001 | Val RMSE: 0.0085 | LR: 0.001000\nEpoch  4/10 | Train Loss: 0.0001 | Val Loss: 0.0002 | Val RMSE: 0.0131 | LR: 0.001000\nEpoch  5/10 | Train Loss: 0.0001 | Val Loss: 0.0001 | Val RMSE: 0.0097 | LR: 0.001000\nEpoch  6/10 | Train Loss: 0.0001 | Val Loss: 0.0001 | Val RMSE: 0.0092 | LR: 0.000500\nEpoch  7/10 | Train Loss: 0.0000 | Val Loss: 0.0001 | Val RMSE: 0.0084 | LR: 0.000500\nEpoch  8/10 | Train Loss: 0.0000 | Val Loss: 0.0001 | Val RMSE: 0.0086 | LR: 0.000500\nEpoch  9/10 | Train Loss: 0.0000 | Val Loss: 0.0001 | Val RMSE: 0.0088 | LR: 0.000500\nEarly stopping triggered at epoch 9\nLoaded best model (Val Loss: 0.0000)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"transformer_model.eval()\nval_preds_scaled = []\nval_true_scaled = []\n\nwith torch.no_grad():\n    for X_batch, c_batch, y_batch in val_loader:\n        X_batch = X_batch.to(device)\n        c_batch = c_batch.to(device)\n        \n        predictions = transformer_model(X_batch, c_batch)\n        \n        val_preds_scaled.extend(predictions.cpu().numpy())\n        val_true_scaled.extend(y_batch.numpy())\n\nval_preds_scaled = np.array(val_preds_scaled)\nval_true_scaled = np.array(val_true_scaled)\n\nprint(f\"Predictions range: {val_preds_scaled.min():.4f} to {val_preds_scaled.max():.4f}\")\nprint(f\"Actual range: {val_true_scaled.min():.4f} to {val_true_scaled.max():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:40:49.881203Z","iopub.execute_input":"2026-01-05T12:40:49.881923Z","iopub.status.idle":"2026-01-05T12:40:54.860523Z","shell.execute_reply.started":"2026-01-05T12:40:49.881894Z","shell.execute_reply":"2026-01-05T12:40:54.859619Z"}},"outputs":[{"name":"stdout","text":"Predictions range: 0.0148 to 0.8091\nActual range: 0.0000 to 1.0000\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Clip predictions to valid range before inverse transform\nval_preds_clipped = np.clip(val_preds_scaled, 0, 1)\n\n# Inverse transform\nval_preds_original = sales_scaler.inverse_transform(\n    val_preds_clipped.reshape(-1, 1)\n).flatten()\n\nval_true_original = sales_scaler.inverse_transform(\n    val_true_scaled.reshape(-1, 1)\n).flatten()\n\nprint(\"Original Scale Results\")\nprint(f\"Predictions range: {val_preds_original.min():,.0f} to {val_preds_original.max():,.0f}\")\nprint(f\"Actual range: {val_true_original.min():,.0f} to {val_true_original.max():,.0f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:40:33.996444Z","iopub.execute_input":"2026-01-05T12:40:33.996803Z","iopub.status.idle":"2026-01-05T12:40:34.004884Z","shell.execute_reply.started":"2026-01-05T12:40:33.996774Z","shell.execute_reply":"2026-01-05T12:40:34.004308Z"}},"outputs":[{"name":"stdout","text":"Original Scale Results\nPredictions range: 616 to 33,620\nActual range: 0 to 41,551\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# RMSE\nrmse_transformer = np.sqrt(mean_squared_error(val_true_original, val_preds_original))\nrmspe_transformer = rmspe(val_true_original, val_preds_original)\n# MAE\nmae_transformer = mean_absolute_error(val_true_original, val_preds_original)\n\n# R² Score\nr2_transformer = r2_score(val_true_original, val_preds_original)\n\n\n\n# Average sales for context\navg_sales = val_true_original.mean()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:41:06.668476Z","iopub.execute_input":"2026-01-05T12:41:06.669162Z","iopub.status.idle":"2026-01-05T12:41:06.680915Z","shell.execute_reply.started":"2026-01-05T12:41:06.669127Z","shell.execute_reply":"2026-01-05T12:41:06.680204Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Model Error Metrics\nprint(f\"RMSE (Root Mean Square Error): {rmse_transformer:>12,.2f} sales units\")\nprint(f\"RMSPE Transformer: {rmspe_transformer:>12,.2f}\")\nprint(f\"R² Score: {r2_transformer:>12.4f}\")\n# Business Context Metrics\nprint(f\"Average Actual Sales: {avg_sales:>12,.2f} units\")\nprint(f\"RMSE as % of Average: {(rmse_transformer/avg_sales)*100:>12.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T12:41:53.485264Z","iopub.execute_input":"2026-01-05T12:41:53.486015Z","iopub.status.idle":"2026-01-05T12:41:53.490451Z","shell.execute_reply.started":"2026-01-05T12:41:53.485958Z","shell.execute_reply":"2026-01-05T12:41:53.489778Z"}},"outputs":[{"name":"stdout","text":"RMSE (Root Mean Square Error):       366.86 sales units\nRMSPE Transformer:         0.06\nR² Score:       0.9903\nAverage Actual Sales:     5,731.55 units\nRMSE as % of Average:         6.40%\n","output_type":"stream"}],"execution_count":22}]}